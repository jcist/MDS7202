{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- \\<Joaqu√≠n Cisternas\\>\n",
    "- \\<Diego Gonz√°lez\\>\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: `\\<https://github.com/jcist/MDS7202\\>`\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class Profiler:\n",
    "    def __init__(self, data):\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Se espera un objeto de tipo pandas DataFrame como entrada\")\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        folder_name = now.strftime(\"%d-%m-%Y\")\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "            os.mkdir(os.path.join(folder_name, \"clean_data\"))\n",
    "        \n",
    "        plots_folder = os.path.join(folder_name, 'plots')\n",
    "        if not os.path.exists(plots_folder):\n",
    "            os.mkdir(plots_folder)\n",
    "\n",
    "        self.data = data\n",
    "        self.output_folder = folder_name\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def get_output_folder(self):\n",
    "        return self.output_folder\n",
    "\n",
    "    def summarize(self, columns_of_interest=None, threshold_outliers=3):\n",
    "        if columns_of_interest is None:\n",
    "            columns_of_interest = self.data.columns\n",
    "\n",
    "        summary_file = os.path.join(self.output_folder, 'summary.txt')\n",
    "\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"Resumen del Dataset:\\n\")\n",
    "\n",
    "            for column in columns_of_interest:\n",
    "                if column in self.data.columns:\n",
    "                    f.write(f\"Variable: {column}\\n\")\n",
    "                    data_column = self.data[column]\n",
    "\n",
    "                    # Reportar el tipo de variable\n",
    "                    f.write(f\"Tipo de Variable: {data_column.dtype}\\n\")\n",
    "\n",
    "                    # Reportar el n√∫mero y porcentaje de valores √∫nicos\n",
    "                    unique_count = data_column.nunique()\n",
    "                    total_count = data_column.count()\n",
    "                    unique_percent = (unique_count / total_count) * 100\n",
    "                    f.write(f\"N√∫mero de Valores √önicos: {unique_count} ({unique_percent:.2f}%)\\n\")\n",
    "\n",
    "                    # Reportar el n√∫mero y porcentaje de valores nulos\n",
    "                    null_count = data_column.isnull().sum()\n",
    "                    null_percent = (null_count / total_count) * 100\n",
    "                    f.write(f\"N√∫mero de Valores Nulos: {null_count} ({null_percent:.2f}%)\\n\")\n",
    "\n",
    "                    # Si la variable es num√©rica\n",
    "                    if pd.api.types.is_numeric_dtype(data_column):\n",
    "                        f.write(\"Estad√≠sticas Descriptivas:\\n\")\n",
    "                        f.write(f\"Valor M√≠nimo: {data_column.min()}\\n\")\n",
    "                        f.write(f\"Percentil 25: {data_column.quantile(0.25)}\\n\")\n",
    "                        f.write(f\"Media: {data_column.mean()}\\n\")\n",
    "                        f.write(f\"Percentil 50 (Mediana): {data_column.median()}\\n\")\n",
    "                        f.write(f\"Percentil 75: {data_column.quantile(0.75)}\\n\")\n",
    "                        f.write(f\"Valor M√°ximo: {data_column.max()}\\n\")\n",
    "\n",
    "                        # Reportar el n√∫mero y porcentaje de valores cero\n",
    "                        zero_count = (data_column == 0).sum()\n",
    "                        zero_percent = (zero_count / total_count) * 100\n",
    "                        f.write(f\"N√∫mero de Valores Cero: {zero_count} ({zero_percent:.2f}%)\\n\")\n",
    "\n",
    "                        # Reportar el n√∫mero y porcentaje de valores negativos\n",
    "                        negative_count = (data_column < 0).sum()\n",
    "                        negative_percent = (negative_count / total_count) * 100\n",
    "                        f.write(f\"N√∫mero de Valores Negativos: {negative_count} ({negative_percent:.2f}%)\\n\")\n",
    "\n",
    "                        # Reportar el n√∫mero y porcentaje de valores at√≠picos (outliers)\n",
    "                        iqr = data_column.quantile(0.75) - data_column.quantile(0.25)\n",
    "                        lower_bound = data_column.quantile(0.25) - threshold_outliers * iqr\n",
    "                        upper_bound = data_column.quantile(0.75) + threshold_outliers * iqr\n",
    "                        outliers_count = ((data_column < lower_bound) | (data_column > upper_bound)).sum()\n",
    "                        outliers_percent = (outliers_count / total_count) * 100\n",
    "                        f.write(f\"N√∫mero de Outliers: {outliers_count} ({outliers_percent:.2f}%)\\n\")\n",
    "\n",
    "                    f.write(\"\\n\")\n",
    "                else:\n",
    "                    f.write(f\"Variable '{column}' no encontrada en el DataFrame.\\n\\n\")\n",
    "    \n",
    "    def plot_vars(self, columns_of_interest=None, top_n_categories=10):\n",
    "        if columns_of_interest is None:\n",
    "            columns_of_interest = self.data.columns\n",
    "\n",
    "        plots_folder = os.path.join(self.output_folder, 'plots')\n",
    "\n",
    "        for column in columns_of_interest:\n",
    "            if column in self.data.columns:\n",
    "                data_column = self.data[column]\n",
    "                plt.figure(figsize=(10, 6))\n",
    "\n",
    "                if pd.api.types.is_numeric_dtype(data_column):\n",
    "                    # Genere un gr√°fico de distribuci√≥n de densidad para variables num√©ricas\n",
    "                    sns.histplot(data_column, kde=True, color='b')\n",
    "                    plt.title(f'Distribuci√≥n de Densidad de {column}')\n",
    "                    plt.xlabel(column)\n",
    "                    plt.ylabel('Densidad')\n",
    "                    file_path = os.path.join(plots_folder, f'{column}.pdf').replace(\"\\\\\", \"/\")\n",
    "                    plt.savefig(file_path)\n",
    "                    plt.close()\n",
    "\n",
    "                    # Grafique la correlaci√≥n entre las variables num√©ricas\n",
    "                    if len(columns_of_interest) > 1:\n",
    "                        for col2 in columns_of_interest:\n",
    "                            if col2 != column and pd.api.types.is_numeric_dtype(self.data[col2]):\n",
    "                                sns.scatterplot(x=column, y=col2, data=self.data)\n",
    "                                plt.title(f'Correlaci√≥n entre {column} y {col2}')\n",
    "                                plt.xlabel(column)\n",
    "                                plt.ylabel(col2)\n",
    "                                file_path = os.path.join(plots_folder, f'{column}_vs_{col2}.pdf').replace(\"\\\\\", \"/\")\n",
    "                                plt.savefig(file_path)\n",
    "                                plt.close()\n",
    "                elif isinstance(data_column.dtype, pd.CategoricalDtype):\n",
    "                    # Genere un histograma de las top N categor√≠as\n",
    "                    top_categories = data_column.value_counts()[:top_n_categories]\n",
    "                    sns.barplot(x=top_categories.index, y=top_categories.values)\n",
    "                    plt.title(f'Histograma de las Top {top_n_categories} Categor√≠as de {column}')\n",
    "                    plt.xlabel(column)\n",
    "                    plt.ylabel('Frecuencia')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    file_path = os.path.join(plots_folder, f'{column}.pdf').replace(\"\\\\\", \"/\")\n",
    "                    plt.savefig(file_path)\n",
    "                    plt.close()\n",
    "\n",
    "                    # Grafique el coeficiente V de Cramer entre las variables categ√≥ricas\n",
    "                    if len(columns_of_interest) > 1:\n",
    "                        for col2 in columns_of_interest:\n",
    "                            if col2 != column and pd.api.types.is_categorical_dtype(self.data[col2]):\n",
    "                                confusion_matrix = pd.crosstab(data_column, self.data[col2])\n",
    "                                chi2, _, _, _ = chi2_contingency(confusion_matrix)\n",
    "                                n = self.data.shape[0]\n",
    "                                v_cramer = np.sqrt(chi2 / (n * (min(confusion_matrix.shape) - 1)))\n",
    "                                sns.heatmap(np.array([[v_cramer]]), annot=True, cmap='coolwarm', fmt=\".2f\", cbar=False)\n",
    "                                plt.title(f'Coeficiente V de Cramer entre {column} y {col2}')\n",
    "                                file_path = os.path.join(plots_folder, f'{column}_vs_{col2}_cramer.pdf').replace(\"\\\\\", \"/\")\n",
    "                                plt.savefig(file_path)\n",
    "                                plt.close()\n",
    "    def clean_data(self, columns_of_interest=None, drop_duplicates=True, impute_technique='drop', separator=','):\n",
    "        if columns_of_interest is None:\n",
    "            columns_of_interest = self.data.columns\n",
    "\n",
    "        clean_data_folder = os.path.join(self.output_folder, 'clean_data')\n",
    "        if not os.path.exists(clean_data_folder):\n",
    "            os.mkdir(clean_data_folder)\n",
    "\n",
    "        # Crear una copia del DataFrame para el procesamiento\n",
    "        cleaned_data = self.data.copy()\n",
    "\n",
    "        # Separar columnas no at√≥micas\n",
    "        for column in columns_of_interest:\n",
    "            if column in cleaned_data.columns:\n",
    "                if separator in cleaned_data[column].iloc[0]:\n",
    "                    new_columns = cleaned_data[column].str.split(separator, expand=True)\n",
    "                    new_columns.columns = [f\"{column}_{i}\" for i in range(new_columns.shape[1])]\n",
    "                    cleaned_data = pd.concat([cleaned_data, new_columns], axis=1)\n",
    "                    cleaned_data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        # Drop de valores duplicados\n",
    "        if drop_duplicates:\n",
    "            cleaned_data.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Tratar valores nulos\n",
    "        if impute_technique == 'drop':\n",
    "            cleaned_data.dropna(subset=columns_of_interest, inplace=True)\n",
    "        elif impute_technique == 'impute_mean':\n",
    "            for column in columns_of_interest:\n",
    "                if column in cleaned_data.columns:\n",
    "                    cleaned_data[column].fillna(cleaned_data[column].mean(), inplace=True)\n",
    "\n",
    "        # Guardar los datos procesados en formato .csv\n",
    "        cleaned_data_path = os.path.join(clean_data_folder, 'data.csv')\n",
    "        cleaned_data.to_csv(cleaned_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'numpy.int64' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joaci\\OneDrive - Universidad de Chile\\XII Semestre\\Lab de programaci√≥n cientifica\\Repo\\MDS7202\\Proyecto.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m profiler \u001b[39m=\u001b[39m Profiler(data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Limpiar los datos y guardarlos en un nuevo archivo CSV\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m profiler\u001b[39m.\u001b[39mclean_data(columns_of_interest\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m], drop_duplicates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, impute_technique\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimpute_mean\u001b[39m\u001b[39m'\u001b[39m, separator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\joaci\\OneDrive - Universidad de Chile\\XII Semestre\\Lab de programaci√≥n cientifica\\Repo\\MDS7202\\Proyecto.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39m# Separar columnas no at√≥micas\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m columns_of_interest:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m     \u001b[39mif\u001b[39;00m column \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcolumns \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[column]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[column] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[column]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)  \u001b[39m# Convertir a cadena\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/joaci/OneDrive%20-%20Universidad%20de%20Chile/XII%20Semestre/Lab%20de%20programaci%C3%B3n%20cientifica/Repo/MDS7202/Proyecto.ipynb#X11sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m         new_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[column]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, expand\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'numpy.int64' is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = pd.DataFrame({'A': ['1,2', '3,4', '5,6', '7,8', '9,10'], 'B': [1, 2, 3, 4, 5]})\n",
    "\n",
    "# Crear una instancia de la clase Profiler\n",
    "profiler = Profiler(data)\n",
    "\n",
    "# Limpiar los datos y guardarlos en un nuevo archivo CSV\n",
    "profiler.clean_data(columns_of_interest=['A', 'B'], drop_duplicates=True, impute_technique='impute_mean', separator=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3,4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5,6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7,8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9,10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B\n",
       "0   1,2  1\n",
       "1   3,4  2\n",
       "2   5,6  3\n",
       "3   7,8  4\n",
       "4  9,10  5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
